\hypertarget{mmdb}{%
\section{mmdb}\label{mmdb}}

\includegraphics{dataflow.png}

\hypertarget{multimodal-database}{%
\subsection{Multimodal Database}\label{multimodal-database}}

\texttt{mmdb} https://github.com/fdch/mmdb is a multimodal database system geared towards live querying of image an audio. A multimodal database combines two sensing modes. In this sense, the camera sensor and the microphone.

The system here enables you to load a folder with images with various formats and sizes, analyze them, and output a database describing the images with some useful keywords (descriptors). The images can be either taken and collected by you, obtained from the web, or generated by some other means. The analysis is done after normalizing these images to the same size and format in a pre-processing step.

After the analysis is done, the database obtained is divided into two types. The first type is a very small text file with only a handful of values that describe a few things of the image. This file is useful to sort all images based on some or all of these values. The second type is a semi-structured JSON file that includes a lot of data referring to each image. This second file is then used for a set of purposes. On the one hand, we can use this database to perform queries based on those values and obtain desired images. For example, we can ask for bright images, or images with faces or bodies, or images with lots of blobs, etc. On the other, from this database we obtain a set of color words (English color names) of the most present colors on each image. These color words become the link between image and audio in a process that goes as follows. First, we use these color words to query related nouns to those colors using an online database called Datamuse.com. Then, from this query, we obtain another database that has all of these colors and nouns. Finally, this intermediary database that has only text is used to query Freesound.org, to match and download sounds related to those words. Once we have our folder with downloaded audio files from Freesound, we concatenate all of these sounds in sound files named with their respective color words.

Now we can use our audio and image databases to perform a simultaneous query to both, and display this live as an audio/visual stream. The live query is made with a matching matrix that equates certain image descriptors with some audio descriptors. For example, images with faces and bodies will match with audios with pitches on them, and images with many blobs will match with noisier sounds.

\hypertarget{steps}{%
\subsection{Steps}\label{steps}}

\hypertarget{download}{%
\subsubsection{1. Download}\label{download}}

Download image dataset (raw, original files) into \texttt{raw} directory.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\hypertarget{preprocess}{%
\subsubsection{2. Preprocess}\label{preprocess}}

\texttt{sh\ preprocess}

Resize, rename, and/or convert raw images into \texttt{img}, \texttt{vid}, and \texttt{aud} directories. This step needs \href{https://ffmpeg.org/ffmpeg.html}{ffmpeg} and \href{https://ss64.com/osx/sips.html}{sips}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\hypertarget{analyze-images}{%
\subsubsection{3. Analyze Images}\label{analyze-images}}

\texttt{sh\ analyze}

Output two files: an entry file and a data file

\hypertarget{a}{%
\paragraph{A}\label{a}}

Entry file contains one entry per image file holding the following data:

\begin{itemize}
\tightlist
\item
  brightness: variance of the image histogram
\item
  bodies : number of bodies found (haarcascades)
\item
  faces : number of faces found (haarcascades)
\item
  cvblobs : number of cvblobs found
\item
  lines : number of hough lines found
\item
  circles : number of hough circles found
\item
  keypoints : number of keypoints (corners) found
\end{itemize}

\hypertarget{b}{%
\paragraph{B}\label{b}}

\texttt{JSON} data files contains one entry per image file holding the actual data

{[}mean\_col (x), histo (64), bodies (x), faces (x), cvblobs (x), lines (x), circles (x), keypoints (x){]}

\hypertarget{optionally}{%
\paragraph{Optionally:}\label{optionally}}

Define a ROI using the \texttt{roi.pd} patch, and then set the ROI flag to 1 before running \texttt{sh\ analyze}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\hypertarget{sorter}{%
\subsubsection{4. Sorter}\label{sorter}}

\texttt{sh\ sorter}

Use A to sort files based on any given field, and pairs of fields

Output sorted files into \texttt{*-sorted.txt} where each line contains the sorted inidices of each image filename.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\hypertarget{color-sounds}{%
\subsubsection{5. Color Sounds}\label{color-sounds}}

Obtain a sound database based on image colors. This is broken down in four steps:

\hypertarget{get-color-words}{%
\paragraph{5.1. Get color words:}\label{get-color-words}}

\texttt{python\ src/colors.py}

First, this script places all data objects (B) inside an array of objects in one \texttt{JSON} object (C) (Concatenates \texttt{JSON} files into one)

This script gets English names of the clustered colors in the \texttt{JSON} data base (C), and outputs a file \texttt{./data/colorwords.json} containing one entry per unique color. The structure is like this: \texttt{name}, \texttt{idlist}, and \texttt{words}.

\begin{itemize}
\tightlist
\item
  \texttt{name} : has the English name of the color, e.g.~`blue'
\item
  \texttt{idlist}: has all the image ids that have that color
\item
  \texttt{words}: has nouns related to such color. These nouns are obtained by querying \href{https://datamuse.com}{datamuse}, e.g.~`sky, eyes, etc.'
\end{itemize}

\hypertarget{get-color-sounds}{%
\paragraph{5.2. Get color sounds:}\label{get-color-sounds}}

The file \texttt{./data/colorwords.json} is then used to query \href{https://freesound.org}{Freesound} and download sounds related to all \texttt{words} and \texttt{name}s using:

\texttt{python\ src/fs\_download.oy}

NOTE: some colors may not result in words that have a related sound to them.

\hypertarget{concatenate-sounds}{%
\paragraph{5.3. Concatenate sounds:}\label{concatenate-sounds}}

Concatenate color sounds into same files and name the file with the image id:

\texttt{python\ src/concat\_sounds.py}

This script runs \texttt{ffprobe} to ignore files that might not be audio, or that might be malformed. It then runs \texttt{ffmpeg} to concatenate all the audio related to a color name into a file named with that same color name.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\hypertarget{analyze-sounds}{%
\subsubsection{6. Analyze Sounds}\label{analyze-sounds}}

\texttt{sh\ analyze\_sounds.sh}

This script runs the \texttt{analyze\_sounds.pd} file in batch mode. It analyzes sounds in a given directory, and places all \texttt{*.timid} files in a second directory. Optionally, you can analyze only one file by index into the directory with a 3rd argument.

By default, the analysis is outputted both in \texttt{*.timid} and in \texttt{*.json} (using \texttt{timid2json.py}), and it concatenates all \texttt{JSON} files into one database.

\hypertarget{instance-structure}{%
\paragraph{Instance Structure}\label{instance-structure}}

The first nine features are single-valued, so one float each. The last two features default to 50 values each, representing the bins of the bark scale with a filterbank spaced at 0.5. You can edit this and other parameters on the parameters file (open \texttt{analyze\_sounds.pd} to do this). The instance length would change accordingly. The output analysis file is one per each audio file, with the following instance structure:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  barkSpecSlope
\item
  barkSpecKurtosis
\item
  barkSpecSkewness
\item
  barkSpecBrightness
\item
  barkSpecFlatness
\item
  barkSpecRolloff
\item
  barkSpecCentroid
\item
  barkSpecSpread
\item
  barkSpecIrregularity
\item
  bfcc
\item
  barkSpec (used for all of the above, internal window size is 512)
\end{enumerate}

(see help files for \href{https://github.com/wbrent/timbreID}{timbreID})

The default analysis window size is 4096, so in one second of file at 44100, you will have around 10 instances, which is ok for many purposes, but you can change this. On the one hand, you can specify overlaps (default 1, no overlap). On the other, you can define an analysis average factor \textbf{f} (default 8). This factor is used to average several smaller sized analysis into one. To do this, we simply take the mean of \textbf{f} consecutive analysis frames within the larger analysis window size.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\hypertarget{live-query}{%
\subsubsection{7. Live Query}\label{live-query}}

This enables you to perform live queries to both images and audio simultaneously, using the same query parameters and a matching matrix.

\hypertarget{instructions-to-open-this-patch}{%
\paragraph{Instructions to open this patch:}\label{instructions-to-open-this-patch}}

Run on three separate terminals:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{sh\ audio} (for the sounds)
\item
  \texttt{sh\ display} (for the images)
\item
  Run the live database: \texttt{python\ src/live\_query.py\ ./txt/images-entries.txt\ ./data/images-data.json\ ./data/audio-data.json\ ./data/colorwords.json\ 5011\ localhost}
\item
  Now open \texttt{live\_query.pd}
\end{enumerate}

\hypertarget{matching-matrix}{%
\paragraph{Matching matrix}\label{matching-matrix}}

\begin{longtable}[]{@{}ll@{}}
\toprule
Image Feature & Audio Feature\tabularnewline
\midrule
\endhead
thres\_\{R,G,B\} & audio database\tabularnewline
thres\_C & audio database\tabularnewline
\{bodies, faces\} & Kurtosis\tabularnewline
\{bodies, faces\}{[}size{]} & Skewness\tabularnewline
brightness & Slope\tabularnewline
smoothness* & grain size (for concatenation)\tabularnewline
cutness* & grain size (for concatenation)\tabularnewline
blobiness & Brightness, Flatness, Rolloff\tabularnewline
skewness* & grain location (for spatialization)\tabularnewline
boundedness & Centroid, Spread\tabularnewline
kontrastedness & Irregularity\tabularnewline
\bottomrule
\end{longtable}

(*) Not used in the audio query

\hypertarget{extra}{%
\subsection{Extra}\label{extra}}

\hypertarget{reader-visualizer}{%
\subsubsection{Reader / Visualizer}\label{reader-visualizer}}

\begin{verbatim}
cd bin
pd reader.pd
\end{verbatim}

This patch can be used to visualize the \texttt{JSON} data files (B)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\hypertarget{image-query-non-realtime}{%
\subsubsection{Image Query (non-realtime)}\label{image-query-non-realtime}}

\begin{verbatim}
cd bin
pd query.pd
\end{verbatim}

NOTE: This patch is a gui for \texttt{src/query.py}.

This patch can be used to:

\begin{itemize}
\tightlist
\item
  perform a query to the \texttt{JSON} database (C) to get indices, based on
\item
  multiple descriptors (color, brightness, smoothness, blobiness, etc.),
\item
  visualize the queries for live editing with the \texttt{sh\ display} program
\end{itemize}

Both input query and its results are stored on \texttt{JSON} files for later use.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\hypertarget{dependencies}{%
\subsection{Dependencies}\label{dependencies}}

\hypertarget{externals}{%
\subsubsection{Externals}\label{externals}}

I have not included binaries within this repo, but you can download the following externals:

Available via \texttt{deken}:

\begin{itemize}
\tightlist
\item
  Gem
\item
  pix\_opencv
\item
  purest\_json
\item
  ggee
\item
  timbreID
\item
  zexy/repack
\end{itemize}

Available via github:

\begin{itemize}
\tightlist
\item
  \href{https://github.com/fdch/fd_lib}{fd\_lib} for {[}iterate{]}, {[}counter{]}
\end{itemize}

\hypertarget{abstractions}{%
\subsubsection{Abstractions}\label{abstractions}}

In the \texttt{bin/lib} directory there are some abstractions made for this repo (prepended with a \texttt{\_}. I also have included these together with some other abstractions as well in the \texttt{pdbin} directory that are taken from \href{https://github.com/fdch/fd_lib}{fd\_lib} and other places. \texttt{pdbin} might not be necessary if you have already installed all the external libraries mentioned above. NOTE: the \texttt{pdbin} directory is not necessary to load the patches, it is just placed there for convenience. Just declare it with \texttt{{[}declare\ -path\ ../pdbin{]}} if you need to use it.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\hypertarget{to-do}{%
\subsection{To do}\label{to-do}}

\begin{itemize}
\tightlist
\item
  implement continuity for images (using histogram clusters)
\item
  filters of type \texttt{NOT} in \texttt{query.py}
\item
  match \textbf{histograms} with \textbf{bfcc} ?
\item
  convert image-data.json to matching matrix parametes to calculate distance between audio-data.json
\end{itemize}
